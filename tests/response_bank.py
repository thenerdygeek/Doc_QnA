"""Response bank: realistic LLM responses for each prompt type.

Generated by analyzing prompt templates and their parsing regex patterns.
Each category has: good responses, degraded responses, and malformed responses.

NOTE: Cody output limit is 4000 tokens for Claude 3.5 Sonnet.
"""

# ---------------------------------------------------------------------------
# Intent Classification
# ---------------------------------------------------------------------------

INTENT_CLASSIFICATION = {
    "good": [
        # Example 1: Clean format with all fields
        """Reasoning: The user asks "how does X work" which describes a flow between components. They want to see the interaction between parts of the system. This is best shown as a diagram.
Intent: DIAGRAM
Sub-type: sequence""",

        # Example 2: Code example request
        """Reasoning: The user says "show me how to call" an endpoint. They want a concrete API call example they can copy. This is a code example request.
Intent: CODE_EXAMPLE
Sub-type: curl""",

        # Example 3: Comparison request
        """Reasoning: The user asks about "differences between" two approaches. They want a structured comparison. This is a comparison request.
Intent: COMPARISON_TABLE
Sub-type: none""",

        # Example 4: Procedural request
        """Reasoning: The user says "how do I set up" which is action-oriented. They want step-by-step instructions to accomplish a configuration task.
Intent: PROCEDURAL
Sub-type: none""",

        # Example 5: Explanation (default)
        """Reasoning: The user asks a factual question about rate limits. They want a direct answer, not a diagram, code, table, or procedure.
Intent: EXPLANATION
Sub-type: none""",
    ],

    "degraded": [
        # Extra preamble text before the expected format
        """Let me think about this query carefully.

The user is asking about how the authentication flow works between multiple services. This suggests they want to understand the interaction pattern.

Reasoning: The query asks about a flow between components. This is best visualized as a sequence diagram showing the message passing.
Intent: DIAGRAM
Sub-type: sequence""",

        # Missing Sub-type field (should default to "none" in parsing)
        """Reasoning: The user wants to know the difference between JWT and sessions. They're looking for a side-by-side comparison.
Intent: COMPARISON_TABLE""",

        # Different separator/formatting
        """Reasoning: User asks for code example
Intent : CODE_EXAMPLE
Sub-type : json""",
    ],

    "malformed": [
        # No structured format at all, just prose - should trigger fuzzy fallback
        """This query is asking about the OAuth flow which is a diagram-related question about how systems interact with each other in a security context.""",

        # Completely wrong format
        """Answer: The user wants a diagram
Type: flowchart
Confidence: high""",
    ],

    "prompt_analysis": """The prompt is clear and provides good examples. However:
1. The "Intent:" and "Sub-type:" parsing is brittle - requires exact format with colon separator.
2. The fuzzy fallback (searching for intent name anywhere in response) is good defensive design.
3. The prompt says 'Think step-by-step' but doesn't enforce the Reasoning/Intent/Sub-type structure strongly enough.
4. Sub-type detection is inconsistent - some intents have rich sub-types (diagram: sequence/ER/class/state, code: curl/graphql/grpc/yaml/json/http) while others just have "none".
5. IMPROVEMENT: Could add explicit formatting instruction like "You MUST respond in this exact format:" before showing the structure."""
}

# ---------------------------------------------------------------------------
# Query Decomposition
# ---------------------------------------------------------------------------

QUERY_DECOMPOSITION = {
    "good": [
        # Example 1: Standard format with SUB-QUERY N:
        """SUB-QUERY 1: Show me a diagram of how the OAuth2 authorization code flow works
SUB-QUERY 2: Provide a code example of implementing OAuth2 in Python""",

        # Example 2: Three sub-queries
        """SUB-QUERY 1: What are the differences between JWT and session-based authentication?
SUB-QUERY 2: Show me how to implement JWT authentication in Node.js
SUB-QUERY 3: Draw a sequence diagram of the JWT authentication flow""",

        # Example 3: Single sub-query (no decomposition needed)
        """SUB-QUERY 1: How do I configure the API rate limiter?""",
    ],

    "degraded": [
        # Numbered list fallback format (N. or N))
        """Based on the query, I'll decompose this into:

1. Draw a sequence diagram showing the OAuth flow between client, auth server, and resource server
2. Provide a curl example of calling the authorization endpoint
3. Explain the security benefits of using OAuth2""",

        # Mixed format with preamble
        """The user is asking for multiple things. Here are the sub-queries:

SUB-QUERY 1: Compare REST and GraphQL APIs
SUB-QUERY 2: Show example GraphQL query syntax""",

        # Numbered with parentheses
        """1) What is the difference between async and sync API calls?
2) Show a code example of making an async API call in JavaScript""",
    ],

    "malformed": [
        # No structured format at all
        """The query can be split into a diagram request and a code example request about OAuth.""",

        # Wrong format entirely
        """Part A: diagram
Part B: code example
Part C: explanation""",
    ],

    "prompt_analysis": """The prompt is reasonably clear but has issues:
1. The regex accepts both "SUB-QUERY N:" and numbered lists "N." or "N)" which is good for robustness.
2. The max_sub_queries parameter is mentioned but not strongly enforced in the prompt text.
3. The instruction "Preserve the original topic/subject in each sub-query" is good but vague.
4. ISSUE: No explicit instruction about the format - just shows an example. Should say "Return EXACTLY this format:".
5. Last resort fallback (return entire response as single query) is good defensive coding."""
}

# ---------------------------------------------------------------------------
# Document Grading
# ---------------------------------------------------------------------------

DOCUMENT_GRADING = {
    "good": [
        # Example 1: Single chunk
        """Chunk 1: RELEVANT — Directly describes the OAuth2 flow with all key steps""",

        # Example 2: Multiple chunks with mixed grades
        """Chunk 1: RELEVANT — Contains the complete API endpoint documentation
Chunk 2: PARTIAL — Mentions authentication but lacks implementation details
Chunk 3: IRRELEVANT — Discusses unrelated database configuration""",

        # Example 3: Five chunks (stress test)
        """Chunk 1: RELEVANT — Full documentation of the rate limiting mechanism
Chunk 2: RELEVANT — Example configuration for rate limits
Chunk 3: PARTIAL — General API documentation that mentions rate limits
Chunk 4: IRRELEVANT — About logging, not rate limiting
Chunk 5: PARTIAL — Performance tuning that indirectly relates to rate limits""",
    ],

    "degraded": [
        # Different separator (dash instead of em-dash)
        """Chunk 1: RELEVANT - Has the info we need
Chunk 2: PARTIAL - Some useful context
Chunk 3: IRRELEVANT - Not related""",

        # Extra text around grades
        """Looking at each chunk:

Chunk 1: RELEVANT — This chunk directly answers the question about JWT structure
Chunk 2: PARTIAL — Contains some authentication info but not JWT-specific

Overall, we have one strong match.""",

        # Case variation
        """Chunk 1: relevant — contains the answer
Chunk 2: IRRELEVANT — wrong topic""",
    ],

    "malformed": [
        # Wrong format entirely
        """1. First chunk is good
2. Second chunk is partially useful
3. Third chunk is not relevant""",

        # Missing chunks
        """All chunks are relevant""",
    ],

    "prompt_analysis": """The prompt is clear and well-structured:
1. Good definition of RELEVANT/PARTIAL/IRRELEVANT with clear semantics.
2. The regex pattern accepts both em-dash (—) and regular dash (-) which is good for robustness.
3. Conservative fallback (missing grade → "relevant") prevents accidentally discarding useful chunks.
4. ISSUE: The prompt doesn't explicitly say "Grade EVERY chunk" which might lead to LLM skipping some.
5. IMPROVEMENT: Could add "You must grade all N chunks" with the actual count."""
}

# ---------------------------------------------------------------------------
# Query Rewriting
# ---------------------------------------------------------------------------

QUERY_REWRITE = {
    "good": [
        # Example 1: Clean rewrite
        """How to configure JWT token expiration in the API gateway authentication module""",

        # Example 2: Incorporates terminology from partial matches
        """OAuth2 authorization code flow implementation with PKCE extension for mobile apps""",

        # Example 3: More specific version of vague query
        """REST API rate limiting configuration using token bucket algorithm""",
    ],

    "degraded": [
        # Extra preamble before the query
        """Based on the partial matches, I'll make the query more specific:

How to implement OAuth2 client credentials flow for service-to-service authentication""",

        # Wrapped in quotes
        '"What is the difference between symmetric and asymmetric JWT signing algorithms"',

        # Multiple sentences (should still work, just uses the whole thing)
        """How do I configure rate limits? Specifically the per-user rate limiting in the API gateway.""",
    ],

    "malformed": [
        # Returns explanation instead of query
        """The query should focus more on the specific OAuth flow type and the implementation details rather than asking generally about authentication.""",

        # Empty response
        """""",
    ],

    "prompt_analysis": """The prompt is simple and clear:
1. Good instruction: "Return ONLY the rewritten query, nothing else."
2. The parsing is trivial (just .strip() the response) which is robust.
3. Fallback to original query on error is sensible.
4. ISSUE: No validation that the rewritten query is actually different or better.
5. ISSUE: No constraint on length - LLM could return a very long query.
6. IMPROVEMENT: Could add examples of good vs. bad rewrites."""
}

# ---------------------------------------------------------------------------
# Verification
# ---------------------------------------------------------------------------

VERIFICATION = {
    "good": [
        # Example 1: Clean PASS
        """Verdict: PASS
Confidence: 0.95
Issues: none
Suggested fix: none""",

        # Example 2: FAIL with issues
        """Verdict: FAIL
Confidence: 0.85
Issues: answer claims OAuth uses JWS tokens but docs only mention JWT, missing information about token expiration
Suggested fix: Replace JWS reference with JWT and add token expiration details from section 3.2""",

        # Example 3: PASS with low confidence
        """Verdict: PASS
Confidence: 0.6
Issues: none
Suggested fix: none""",

        # Example 4: FAIL with multiple issues
        """Verdict: FAIL
Confidence: 0.9
Issues: hallucinated endpoint /auth/login not in docs, wrong rate limit value (100 vs 50), omitted error codes
Suggested fix: Correct endpoint to /api/v1/authenticate, fix rate limit to 50 req/min, add error codes 401 and 429""",

        # Example 5: Edge case - 0.0 confidence
        """Verdict: FAIL
Confidence: 0.0
Issues: completely unsupported by sources
Suggested fix: Rewrite based on actual documentation""",
    ],

    "degraded": [
        # Extra text around structured fields
        """Let me verify this answer carefully.

After checking against the source documents:

Verdict: FAIL
Confidence: 0.75
Issues: Missing prerequisite step about installing dependencies
Suggested fix: Add step 1 about running npm install

The answer is mostly correct but incomplete.""",

        # Different spacing/formatting
        """Verdict:PASS
Confidence : 0.88
Issues :none
Suggested fix: none""",

        # Confidence as percentage
        """Verdict: PASS
Confidence: 85%
Issues: none
Suggested fix: none""",
    ],

    "malformed": [
        # No structured format
        """The answer looks mostly correct but it's missing some details about error handling. I'd estimate about 70% confidence in the answer.""",

        # Wrong field names
        """Result: PASS
Certainty: 0.9
Problems: none
Recommendation: none""",
    ],

    "prompt_analysis": """The prompt is well-designed with clear structure:
1. Good checklist of what to verify (hallucinations, omissions, factual accuracy).
2. Clear format specification with "Respond in this exact format:".
3. Regex patterns are flexible (case-insensitive, handles spacing variations).
4. Good fallback logic: missing verdict → PASS (conservative), missing confidence → 0.5, missing issues → "verification parse error".
5. ISSUE: Confidence parsing doesn't handle percentage format (e.g., "85%") - would fail to parse.
6. ISSUE: "Issues: none" vs. "Issues: " (empty) are treated differently by regex but should be equivalent.
7. IMPROVEMENT: Could specify confidence range more clearly (0.0 = no confidence, 1.0 = certain)."""
}

# ---------------------------------------------------------------------------
# Diagram Generation
# ---------------------------------------------------------------------------

DIAGRAM_GENERATION = {
    "good": [
        # Example 1: Flowchart
        """Here's a flowchart showing the authentication process:

```mermaid
graph TD
    Start[User Login Request] --> Validate{Valid Credentials?}
    Validate -->|Yes| GenerateToken[Generate JWT Token]
    Validate -->|No| Error[Return 401 Error]
    GenerateToken --> Return[Return Token to Client]
    Error --> End
    Return --> End
```

This diagram illustrates the basic authentication flow. When a user submits login credentials, the system validates them. If valid, a JWT token is generated and returned. If invalid, a 401 error is returned.""",

        # Example 2: Sequence diagram
        """Here's a sequence diagram showing the OAuth2 flow:

```mermaid
sequenceDiagram
    participant User
    participant Client
    participant AuthServer
    participant ResourceServer

    User->>Client: 1. Request Resource
    Client->>AuthServer: 2. Authorization Request
    AuthServer->>User: 3. Login Page
    User->>AuthServer: 4. Credentials
    AuthServer->>Client: 5. Authorization Code
    Client->>AuthServer: 6. Token Request + Code
    AuthServer->>Client: 7. Access Token
    Client->>ResourceServer: 8. API Call + Token
    ResourceServer->>Client: 9. Protected Resource
    Client->>User: 10. Display Resource
```

The diagram shows the complete authorization code flow with all parties involved.""",

        # Example 3: Multiple diagrams
        """Here are two diagrams to illustrate the system:

```mermaid
graph LR
    A[API Gateway] --> B[Auth Service]
    A --> C[User Service]
    A --> D[Data Service]
    B --> E[(User DB)]
    C --> E
    D --> F[(Main DB)]
```

Architecture overview showing service dependencies.

```mermaid
stateDiagram-v2
    [*] --> Unauthenticated
    Unauthenticated --> Authenticated: Login Success
    Authenticated --> Unauthenticated: Logout
    Authenticated --> Authenticated: Refresh Token
```

User authentication state machine.""",
    ],

    "degraded": [
        # Missing language tag on fence (should still be extractable)
        """Here's the diagram:

```
graph TD
    A[Start] --> B[Process]
    B --> C[End]
```

This shows the basic flow.""",

        # Invalid Mermaid syntax (tests repair loop)
        """```mermaid
graph TD
    start --> end
    end --> finish
```

Note: Using reserved keyword "end" as node name will cause validation error.""",

        # Extra text before/after
        """I'll create a diagram for you.

First, let me explain what it will show...

```mermaid
flowchart TD
    A --> B
```

As you can see from the diagram, A leads to B.

Hope this helps!""",
    ],

    "malformed": [
        # No mermaid fence at all
        """The flow goes from the client to the auth server, then to the resource server, and back to the client.""",

        # Wrong fence type
        """```diagram
A -> B -> C
```""",
    ],

    "prompt_analysis": """The prompt is detailed with good syntax guidance:
1. Specifies diagram type via {diagram_type} parameter which is good.
2. Includes specific Mermaid syntax rules (escape semicolons, avoid bare "end", etc.) which shows awareness of common errors.
3. Validation and repair loop (up to 3 attempts) is excellent engineering.
4. ISSUE: "Wrap the diagram in a ```mermaid code fence" should say "```mermaid" more explicitly.
5. ISSUE: Doesn't specify whether to include explanation text outside the fence (examples do, but prompt doesn't require it).
6. IMPROVEMENT: Could include example of a valid diagram for the specified type."""
}

# ---------------------------------------------------------------------------
# Code Example Generation
# ---------------------------------------------------------------------------

CODE_EXAMPLE_GENERATION = {
    "good": [
        # Example 1: curl with language tag
        """Here's how to call the authentication endpoint:

```bash
curl -X POST https://api.example.com/auth/login \\
  -H "Content-Type: application/json" \\
  -d '{
    "username": "user@example.com",
    "password": "SecurePass123!"
  }'
```

This will return a JWT token in the response.""",

        # Example 2: JSON example
        """Example request payload:

```json
{
  "grant_type": "authorization_code",
  "code": "abc123xyz789",
  "redirect_uri": "https://client.example.com/callback",
  "client_id": "my-client-id",
  "client_secret": "my-client-secret"
}
```

Make sure to replace the client credentials with your actual values.""",

        # Example 3: Python code
        """Here's a complete example:

```python
import requests

def authenticate(username, password):
    url = "https://api.example.com/auth/login"
    headers = {"Content-Type": "application/json"}
    payload = {
        "username": username,
        "password": password
    }

    response = requests.post(url, json=payload, headers=headers)

    if response.status_code == 200:
        token = response.json()["access_token"]
        return token
    else:
        raise Exception(f"Authentication failed: {response.status_code}")

# Usage
token = authenticate("user@example.com", "SecurePass123!")
print(f"Token: {token}")
```

This function handles authentication and returns the JWT token.""",
    ],

    "degraded": [
        # Bare fence without language tag (triggers _ensure_language_tags fix)
        """Here's the example:

```
curl -X GET https://api.example.com/users \\
  -H "Authorization: Bearer YOUR_TOKEN_HERE"
```

Replace YOUR_TOKEN_HERE with your actual token.""",

        # Multiple code blocks, some without tags
        """First, set up the configuration:

```
{
  "api_key": "your-key-here",
  "base_url": "https://api.example.com"
}
```

Then make the API call:

```javascript
fetch(config.base_url + '/users', {
  headers: { 'Authorization': 'Bearer ' + config.api_key }
})
```""",

        # Has placeholder values (prompt says not to use, but LLM might anyway)
        """```bash
curl -X POST https://api.example.com/auth \\
  -H "Authorization: Bearer <YOUR_TOKEN>" \\
  -d '{"user_id": "<USER_ID>"}'
```""",
    ],

    "malformed": [
        # No code fence at all
        """To call the endpoint, send a POST request to /auth/login with your credentials in the body.""",

        # Wrong format
        """Code:
curl -X GET https://api.example.com/data""",
    ],

    "prompt_analysis": """The prompt is concise but has gaps:
1. Good: Specifies format via {code_format} parameter.
2. Good: "Use realistic values, not placeholders like <YOUR_TOKEN>" is explicit.
3. Good: Post-processing _ensure_language_tags() fixes bare fences.
4. ISSUE: Very short prompt - might not be enough guidance for complex examples.
5. ISSUE: Doesn't specify whether to include explanation text around the code.
6. ISSUE: "If showing an API call, include the full request with headers and body" - but what if it's not an API call?
7. IMPROVEMENT: Could include example of well-formatted code for the specified language."""
}

# ---------------------------------------------------------------------------
# Comparison Table
# ---------------------------------------------------------------------------

COMPARISON_TABLE = {
    "good": [
        # Example 1: Clean markdown table
        """| Feature | JWT | Session-Based |
|---------|-----|---------------|
| Storage | Client-side (token) | Server-side (session store) |
| Scalability | Highly scalable (stateless) | Less scalable (stateful) |
| Security | Token can't be revoked easily | Sessions can be invalidated |
| Overhead | Larger payload per request | Smaller cookie, DB lookup needed |

**Summary:** JWT is better for distributed systems and microservices due to its stateless nature, while session-based auth provides better control over active sessions and revocation.""",

        # Example 2: More rows
        """| Aspect | REST | GraphQL |
|--------|------|---------|
| Request Structure | Multiple endpoints | Single endpoint |
| Data Fetching | Over-fetching common | Precise data requests |
| Versioning | URL versioning (v1, v2) | Schema evolution |
| Caching | HTTP caching works well | Requires custom caching |
| Learning Curve | Easier for beginners | Steeper learning curve |
| Type Safety | Manual validation needed | Built-in type system |

**Key Differences:** GraphQL excels at reducing over-fetching and under-fetching, while REST benefits from better caching and simpler implementation.""",

        # Example 3: With alignment markers
        """| Feature | Option A | Option B |
|:--------|:--------:|---------:|
| Cost | $10/mo | $50/mo |
| Storage | 10 GB | 100 GB |
| Users | 5 | Unlimited |

Option B provides better value for larger teams.""",
    ],

    "degraded": [
        # Table without alignment row formatting
        """| Feature | A | B |
|---------|---|---|
| Speed | Fast | Slow |
| Cost | Low | High |

Summary: A is faster and cheaper.""",

        # Extra text before table
        """Here's a comparison of the two approaches:

First, let me explain the context...

| Approach | Pros | Cons |
|----------|------|------|
| Sync | Simple | Blocking |
| Async | Non-blocking | Complex |

As shown above, each has tradeoffs.""",

        # Missing summary
        """| Method | Security | Performance |
|--------|----------|-------------|
| HMAC | High | Fast |
| RSA | Very High | Slower |""",
    ],

    "malformed": [
        # No table at all
        """JWT is stateless and scalable, while session-based auth is stateful and easier to revoke. JWT has larger payloads, sessions require server-side storage.""",

        # Wrong format
        """Comparison:
- JWT: stateless, scalable
- Sessions: stateful, revocable""",
    ],

    "prompt_analysis": """The prompt is very minimal:
1. Clear requirements: "Markdown table with clear column headers" and "at least 3 comparison dimensions (rows)".
2. Good addition: "Add a brief summary after the table highlighting the key differences."
3. Validation only checks for table presence (header + separator regex), not quality.
4. ISSUE: Doesn't specify column structure - could be 2-way, 3-way, or N-way comparison.
5. ISSUE: No guidance on what makes a good comparison dimension.
6. ISSUE: Very short prompt - LLM has a lot of freedom in interpretation.
7. IMPROVEMENT: Could include example table to show expected format."""
}

# ---------------------------------------------------------------------------
# Procedural
# ---------------------------------------------------------------------------

PROCEDURAL = {
    "good": [
        # Example 1: Clean numbered steps
        """1. Install the required dependencies by running `npm install express jsonwebtoken`.
2. Create a new file `auth.js` and import the required modules.
3. Configure the JWT secret in your environment variables as `JWT_SECRET`.
4. Implement the login endpoint that accepts username and password.
5. Verify credentials against your user database.
6. Generate a JWT token using `jwt.sign()` with the user payload and secret.
7. Return the token to the client with a 200 status code.
8. Test the endpoint using curl or Postman to verify the token is generated correctly.

**Expected outcome:** You should receive a valid JWT token that can be decoded at jwt.io.""",

        # Example 2: With prerequisites
        """**Prerequisites:**
- Node.js 16+ installed
- Access to the API gateway configuration

**Steps:**

1. Log in to the API gateway admin console.
2. Navigate to Settings → Rate Limiting.
3. Click "Add New Rule".
4. Set the rule name to "Default API Limit".
5. Configure the limit to 100 requests per minute.
6. Select the endpoints to apply the rule to (or choose "All endpoints").
7. Save the rule and wait 30 seconds for it to propagate.
8. Verify the rule is active by checking the Rules list.
9. Test the rate limit by making rapid API calls and confirming you get a 429 response after 100 requests.

**Verification:** The API should return HTTP 429 (Too Many Requests) when the limit is exceeded.""",

        # Example 3: Short procedure
        """1. Open the configuration file `config.yaml`.
2. Update the `timeout` value to `30000` (milliseconds).
3. Save the file and restart the service.

The new timeout will take effect immediately upon restart.""",
    ],

    "degraded": [
        # Steps without clear numbering at the start
        """First, you need to install the dependencies:

Step 1: Run npm install

Then configure the environment:

Step 2: Add JWT_SECRET to .env file

Finally, implement the endpoint:

Step 3: Create the /auth/login route""",

        # Mixed format (some numbered, some not)
        """1. Install dependencies
2. Configure environment variables

Next, implement the authentication logic.

3. Create the auth middleware
4. Test the implementation""",

        # No numbered list but has procedural content
        """To set up OAuth:

First, register your application with the OAuth provider to get client credentials.

Then, implement the authorization redirect to send users to the OAuth login page.

After that, handle the callback to exchange the authorization code for an access token.

Finally, store the token securely and use it for API requests.""",
    ],

    "malformed": [
        # No steps at all
        """You need to configure the API gateway with rate limiting and then test that it works properly.""",

        # Wrong format
        """A. Install dependencies
B. Configure settings
C. Test the setup""",
    ],

    "prompt_analysis": """The prompt is very minimal:
1. Clear requirement: "Present the answer as numbered steps. Each step should be actionable."
2. Good additions: "Include any prerequisite steps at the beginning" and "expected outcomes or verification steps where relevant."
3. Validation only checks for presence of numbered list (e.g., "1. "), not quality.
4. ISSUE: Doesn't specify numbering format - "1." vs "1)" vs "Step 1:" - but regex only matches "N.".
5. ISSUE: No guidance on step granularity (too fine-grained vs too coarse).
6. ISSUE: Very short prompt - LLM has significant freedom.
7. IMPROVEMENT: Could include example to show good step structure and actionable language."""
}

# ---------------------------------------------------------------------------
# Truncated Responses (4000 token limit testing)
# ---------------------------------------------------------------------------

TRUNCATED_RESPONSES = {
    "good": [
        # Simulated near-limit response (realistic for verification with long source docs)
        """Verdict: PASS
Confidence: 0.92
Issues: none
Suggested fix: none

[Note: This represents a response that includes extensive context or reasoning that approaches the 4000 token limit but still fits within it. The actual response would be much longer with detailed analysis of each claim against source documents.]""",
    ],

    "degraded": [
        # Truncated mid-field (tests parsing robustness)
        """Verdict: PASS
Confidence: 0.88
Issues: The answer claims that the rate limit is 1000 requests per hour but the documentation states it is 1000 requests per day, also missing information about the retry-after header, and the error code should be 429 not 503, additionally the answer does not mention the exponential backoff requirement which is clearly stated in section""",

        # Truncated in middle of table (for comparison)
        """| Feature | JWT | Session |
|---------|-----|---------|
| Storage | Client | Server |
| Scalability | High | Medium |
| Security | Tokens can't be easily revoked | Sessions can be terminated |
| Overhead | Larger payload size per request due to token transmission | Smaller cookie size but requires database""",

        # Truncated diagram
        """```mermaid
sequenceDiagram
    participant User
    participant Client
    participant AuthServer
    participant ResourceServer
    participant Database
    participant Cache
    participant MessageQueue
    participant NotificationService

    User->>Client: Request Resource
    Client->>Cache: Check Cached Token
    Cache-->>Client: Token Not Found
    Client->>AuthServer: Authorization Request
    AuthServer->>Database: Validate Client
    Database-->>AuthServer: Client Valid
    AuthServer->>User: Login""",
    ],

    "malformed": [
        # Completely cut off (no structured data extracted)
        """Here's a detailed comparison of the two authentication methods.

First, let's look at JWT (JSON Web Tokens). JWT is a compact, URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or integrity protected with a Message""",
    ],

    "prompt_analysis": """Truncation handling varies by prompt type:
1. VERIFICATION: Fields are parsed independently, so truncation in "Issues" or "Suggested fix" won't break Verdict/Confidence parsing.
2. DIAGRAM: Truncated mermaid blocks will fail validation and trigger repair loop, which is good.
3. COMPARISON_TABLE: Truncated tables will fail regex match (no closing separator row), triggering warning but returning text as-is.
4. PROCEDURAL: Truncated numbered lists will parse up to truncation point, which is acceptable.
5. INTENT_CLASSIFICATION: Truncation unlikely (short responses), but fuzzy fallback would catch it.
6. QUERY_DECOMPOSITION: Truncated SUB-QUERY lines will parse up to truncation point.
7. ISSUE: No explicit handling for detecting truncation (checking for incomplete sentences, unclosed fences, etc.).
8. IMPROVEMENT: Could add truncation detection and request continuation or fallback to shorter prompt."""
}

# ---------------------------------------------------------------------------
# Additional Edge Cases
# ---------------------------------------------------------------------------

EDGE_CASES = {
    "intent_classification_no_reasoning": {
        # Missing Reasoning field (should still parse Intent and Sub-type)
        "response": """Intent: CODE_EXAMPLE
Sub-type: python""",
        "expected_intent": "CODE_EXAMPLE",
        "expected_confidence": 0.70,  # Lower confidence without reasoning
        "expected_subtype": "python",
    },

    "verification_edge_confidences": {
        # Test confidence edge values
        "responses": [
            """Verdict: PASS
Confidence: 0.0
Issues: none
Suggested fix: none""",

            """Verdict: FAIL
Confidence: 1.0
Issues: complete hallucination
Suggested fix: rewrite entirely""",

            """Verdict: PASS
Confidence: 2.5
Issues: none
Suggested fix: none""",  # Should clamp to 1.0

            """Verdict: FAIL
Confidence: -0.3
Issues: negative confidence test
Suggested fix: none""",  # Should clamp to 0.0
        ],
    },

    "grading_missing_chunks": {
        # Test when LLM skips grading some chunks
        "query": "How does OAuth work?",
        "num_chunks": 5,
        "response": """Chunk 1: RELEVANT — Full OAuth documentation
Chunk 3: PARTIAL — Mentions OAuth but no details
Chunk 5: IRRELEVANT — About database config""",
        "expected_behavior": "Chunks 2 and 4 should default to 'relevant' (conservative fallback)",
    },

    "diagram_with_comments": {
        # Mermaid diagram with comments
        "response": """```mermaid
%% This is a comment
graph TD
    A[Start] --> B[Process]
    B --> C{Decision}
    %% Another comment
    C -->|Yes| D[Success]
    C -->|No| E[Retry]
```

The diagram shows a retry pattern.""",
        "expected_behavior": "Should extract diagram including comments (Mermaid supports %% comments)",
    },

    "code_multiple_fences": {
        # Multiple code blocks of different types
        "response": """First, create the config:

```json
{"api_key": "your-key"}
```

Then use it in your code:

```python
import json
config = json.load(open('config.json'))
```

Finally, test with curl:

```bash
curl -H "Authorization: Bearer ${config.api_key}" https://api.example.com
```""",
        "expected_behavior": "_ensure_language_tags should preserve existing tags and only fix bare fences",
    },

    "decomposition_exceeds_max": {
        # LLM returns more sub-queries than max_sub_queries
        "response": """SUB-QUERY 1: First question about OAuth flow
SUB-QUERY 2: Second question about JWT structure
SUB-QUERY 3: Third question about token refresh
SUB-QUERY 4: Fourth question about revocation
SUB-QUERY 5: Fifth question about security best practices""",
        "max_sub_queries": 3,
        "expected_behavior": "Should cap at first 3 sub-queries",
    },
}

# ---------------------------------------------------------------------------
# Summary Statistics
# ---------------------------------------------------------------------------

RESPONSE_BANK_STATS = {
    "total_prompt_types": 8,
    "responses_per_type": {
        "good": "2-5 examples",
        "degraded": "2-3 examples",
        "malformed": "1-2 examples",
    },
    "total_responses": 60,  # Approximate count
    "coverage": [
        "All parsing regex patterns tested",
        "Fallback logic tested (degraded/malformed cases)",
        "Edge cases for confidence values, truncation, missing fields",
        "Multi-value cases (multiple chunks, multiple diagrams, multiple code blocks)",
    ],
    "prompt_quality_assessment": {
        "clear_and_robust": ["DOCUMENT_GRADING", "VERIFICATION", "QUERY_REWRITE"],
        "needs_improvement": ["CODE_EXAMPLE_GENERATION", "COMPARISON_TABLE_GENERATION", "PROCEDURAL_GENERATION"],
        "moderate": ["INTENT_CLASSIFICATION", "QUERY_DECOMPOSITION"],
    },
}
